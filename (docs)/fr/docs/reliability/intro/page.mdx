export const metadata = { sidebar_position: 1, title: "ğŸŸ¢ Introduction" };

# ğŸŸ¢ Introduction

Ce chapitre couvre comment rendre les complÃ©tions plus fiables, ainsi que la maniÃ¨re d'implÃ©menter des contrÃ´les pour assurer la fiabilitÃ© des rÃ©sultats.

Dans une certaine mesure, la plupart des techniques prÃ©cÃ©demment abordÃ©es ont pour but d'amÃ©liorer la prÃ©cision des complÃ©tions, et donc leur fiabilitÃ©, en particulier l'auto-consistance(@wang2022selfconsistency). Cependant, il existe un certain nombre d'autres techniques qui peuvent Ãªtre utilisÃ©es pour amÃ©liorer la fiabilitÃ©, au-delÃ  des stratÃ©gies de prompting de base.

Les <Term term="LLM">LLM</Term> se sont rÃ©vÃ©lÃ©s Ãªtre plus fiables que ce que nous pourrions attendre en interprÃ©tant ce qu'un prompt _essaie_ de dire lorsqu'ils rÃ©pondent Ã  des prompts mal orthographiÃ©s, mal formulÃ©s ou mÃªme activement trompeurs(@webson2023itscomplicated). MalgrÃ© cette capacitÃ©, ils prÃ©sentent encore divers problÃ¨mes, y compris des hallucinations(@ye2022unreliability), des explications erronÃ©es avec les mÃ©thodes de %%CoT|CoT prompting%%(@ye2022unreliability), et de multiples biais, y compris le biais de l'Ã©tiquette majoritaire, le biais de rÃ©cence et le biais de jeton commun(@zhao2021calibrate). De plus, le CoT en zero-shot peut Ãªtre particuliÃ¨rement biaisÃ© lorsqu'il traite de sujets sensibles(@shaikh2022second).

Parmi les solutions courantes Ã  certains de ces problÃ¨mes figurent les calibrateurs pour Ã©liminer les biais _a priori_, et les vÃ©rificateurs pour Ã©valuer les complÃ©tions, ainsi que la promotion de la diversitÃ© dans les complÃ©tions.
