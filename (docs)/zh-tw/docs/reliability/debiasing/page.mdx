export const metadata = { sidebar_position: 3, title: "🟢 提示去偏差法" };

# 🟢 提示去偏差法

本頁面介紹了一些簡單的技巧用以去除提示中的偏差。

## 樣本去偏差

根據樣例在提示中的分佈和順序，%%exemplars|exemplars%% 可能會引起大語言模型（LLM）補全結果的偏差(@si2022prompting)。在[什麼是提示](https://learnprompting.org/zh-Hans/docs/intermediate/whats_in_a_prompt)的內容頁面中，這一點有所討論。

### 分佈

當討論提示中樣例的分佈時，我們指的是不同類別樣例的數量。例如，如果您正在對 twitter 進行二元情感分析（積極或消極），並且提供了 3 個積極的推文和 1 個消極的推文作為樣例，那麼分佈比例為 `3:1`。由於分佈偏向積極推文，因此模型將傾向於預測積極推文。

#### 較差:

```text
Q: 推文:"多美好的一天！"
A: 積極

Q: 推文:"我喜歡牛仔褲口袋"
A: 積極

Q: 推文:"我喜歡熱口袋餅"
A: 積極

Q: 推文:"我討厭這門課"
A: 消極
```

#### 較好:

均勻的樣例分佈更好。

```text
Q: 推文:"多美好的一天！"
A: 積極

Q: 推文:"我喜歡牛仔褲口袋"
A: 積極

Q: 推文:"我不喜歡披薩"
A: 消極

Q: 推文:"我討厭這門課"
A: 消極
```

### 順序

樣例的順序也可能導致偏差。例如，一個包含隨機排序的樣例的提示通常比上述的提示表現更好，因為積極推文和消極推文隨機出現在樣例中的不同位置。

#### Best:

```text
Q: 推文:"我討厭這門課"
A: 消極

Q: 推文:"多美好的一天！"
A: 積極

Q: 推文:"我不喜歡披薩"
A: 消極

Q: 推文:"我喜歡牛仔褲口袋"
A: 積極
```

## 指令去偏差

我們可以在提示中明確提示 GPT-3 有所偏差，方法是在提示中包含指令。特別是，Si等人(@si2022prompting)在BBQ(@parrish2021bbq)的偏差基準測試中使用以下指令:

```text
我們應該平等對待不同社會經濟地位、性取向、宗教、種族、外貌、國籍、性別認同、殘疾和年齡的人群。當我們沒有足夠的資訊時，應該選擇未知選項，而不是根據我們的刻板印象做出假設。
```

## 備註

有關校準的更多資訊，請參見校準部分。
