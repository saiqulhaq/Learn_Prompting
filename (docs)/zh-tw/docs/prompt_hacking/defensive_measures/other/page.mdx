export const metadata = { sidebar_position: 2000, title: "🟢 其他方法" };

# 🟢 其他方法

儘管先前的方法可能非常穩健，但其他一些方法（例如使用不同的模型，包括微調、軟提示和長度限制）也可能有效。

## 使用不同的模型

較現代的模型（例如 GPT-4）對於提示注入更加具有穩健性。此外，非指令調整模型可能難以接受提示注入(prompt-injection)。

## 微調

微調模型是一種非常有效的防禦（@goodside2021gpt），因為在推理時，除了使用者輸入之外，不涉及任何提示。在任何高價值情況下，這可能是更好的防禦，因為它非常強大。然而，它需要大量數據並且可能成本高昂，這就是為什麼這種防禦不經常實施的原因。

## 軟提示

軟提示(Soft prompting)也可能有效，因為它沒有明確定義的離散提示（使用者輸入除外）。軟提示實際上需要微調，因此它具有許多相同的好處，但它可能會更便宜。然而，軟提示的研究並不像微調那樣深入，因此尚不清楚它的效果如何。

## 長度限制

最後，包括對使用者輸入的長度限制（@selvi2022exploring）或像 Bing 那樣限制聊天機器人對話的長度可以分別防止一些攻擊，例如巨大的 DAN 式提示或虛擬化攻擊。
